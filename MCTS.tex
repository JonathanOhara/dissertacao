\chapter{Árvore de jogos}
\label{chap:arvoreDeJogos}

\section{Introdução}

Nesse capítulo vamos discutir sobre tomada de decisão em jogos e árvore de decisões. Em árvores de decisões nós focaremos em 2 diferentes métodos de buscas: técnicas baseadas em \textit{minimax} e técnicas baseadas em Monte Carlo.

O capítulo a seguir está organizado do seguinte modo. Primeiro, a seção \ref{sec:tomadaDecisao} irá explicar o básico da idéia de tomada de decisão e os elementos envolvidos nela. Na seção \ref{sec:arvoreDecisao} demonstra como decisões podem ser modeladas em computação. Em \ref{sec:minimax} será discutido o conceito de \textit{minimax}, algumas técnicas e melhorias. Na seção \ref{sec:MCTS} irá expor sobre o método de Monte Carlo e sua aplicação em árvores de busca. Por fim em \ref{sec:asdasd} será discutido alguns trabalhos relacionados a árvore de busca de Monte-Carlo.

\section{Tomada de decisão}
\label{sec:tomadaDecisao}

É muito comum associar inteligência para jogos, mais especificadamente inteligência artificial para jogos, com a habilidade em que os elementos do jogo (jogadores não humanos aliados ou inimigos, objetos, fases por exemplo) tem para tomar decisões dados certas situações. Apesar disso, a tomada de decisão não é o único componente de uma inteligência artificial, na seção \ref{sec:tomadaDecisao} vamos discutir mais profundamente sobre tomada de decisão e de outros elementos que permeiam essa área.

Tomada de decisão é o processo de escolher uma ação entre diversas possibilidades. Segundo \cite{introductionDecisionMaking} "Tomada de decisão é o estudo do processo de identificar e escolher alternativas baseadas nas preferências do tomador de decisões. Tomar uma decisão implica que existem diferentes escolhas para serem consideradas, e, em tal caso, não queremos apenas identificar quais dessas alternativas são viáveis, mas escolher a decisão que melhor se encaixa com as nossas metas, objetivos, desejos, valores, e assim por diante."

Ainda na teoria do processo de tomada de decisão, \cite{guidebookDecisionMaking} diz que o primeiro passo na tomada de decisão é estabelecer quem é(são) o(s) tomador(es) de decisão e o(s) \textit{stakeholder(s)} (partes afetadas ou interessadas), de modo a mitigar um possível desacordo sobre a definição do problema, metas e critérios. O processo pode ser definidos no seguintes passos:

\begin{itemize}
	
	\item Definir o problema;
	
	\item Determinar os requisitos que a solução deve apresentar;

	\item Estabelecer objetivos que a solução do problema deve realizar;
	
	\item Identificar alternativas que irão solucionar o problema;
	
	\item Desenvolver critérios de avaliação com base nos objetivos;
	
	\item Selecionar uma ferramenta ou método para de decisão;
	
	\item Aplicar a ferramenta ou método para selecioanar a alternativa preferida;
	
	\item Validar se a resposta resolveu o problema.
	
\end{itemize}

Nos jogos, segundo \cite{Millington:2009:AIG:1795711} a entrada da tomada de decisão é o conhecimento que tal personagem tem e a saída é a ação a ser realizada. O conhecimento pode ser dividio em interno e externo. Conhecimento externo é a informação que o personagem tem sobre o ambiente em sua volta: posição dos outros personagens, o leiaute da fase, se um interruptor foi ligado, a direção que um barulho veio, e assim por diante. Conhecimento interno é a informação sobre o estado interno do personagem ou pensamentos internos como: sua saúde, objetivos, seu passado, e assim por diante. Ainda segundo \cite{Millington:2009:AIG:1795711} a representação do conhecimento está intrinsecamente ligado com a maioria dos algoritmos de tomada de decisão.

\subsection{Árvore de decisão}
\label{sec:arvoreDecisao}

Árvores de decisão é uma maneira muito clara e natural de representar uma cadeia de decisões e suas consequências. Segundo \cite{Millington:2009:AIG:1795711} árvores de decisão são rápidas, facéis de implementar, e simples de entender. Elas são a técnica mais simples de tomada de decisão. Outra vantagem é a questão de ser modular e muito fácil de ser criada.

Um modo muito comum de representar árvores de decisão para estratégias simples é o fluxograma. Segundo \cite{Millington:2009:AIG:1795711} as decisões árvore de decisões representadas por fluxogramas tem normalmente duas resposta (sim e não por exemplo). Na imagem \ref{fig:fluxogramaInimigos} é mostrado um diagrama simples de estratégia para um inimigo, nesse fluxograma o personagem fica patrulhando até achar um inimigo, ao ver um inimigo o personagem se aproxima se necessário e o ataca.

\begin{figure}[!h]
\centering
\includegraphics[width=12cm]{figures/fluxogramainimigo.png}
\caption[Fluxograma estratégia inimigo]{Fluxograma Simples de estratégia para inimigos.}
\label{fig:fluxogramaInimigos}
\end{figure}

\subsection{Máquina de estados finitos}
\label{sec:fsm}

Outro modo de representar comportamentos de certos elementos é através de máquinas de estados finitos (do inglês Finite State Machine - FSM). \cite{wright2012finite} define o modelo conceitual de FSM da seguinte forma:

\begin{itemize}
	
	\item O sistema (objeto que será modelado como uma FSM) deve ser descrevível como uma máquina finita de estados.
	
	\item O sistema precisa ter um número finito de entradas e/ou eventos que podem disparar transações entre estados.

	\item O comportamento do sistema dado um ponto no tempo ddo estado atual e da entrada ou evento que ocorrerá naquele tempo.
	
	\item Para cada possível estado do sistema, o comportamento é definido para cada entrada ou evento possível.
	
	\item O sistema tem um estado inicial particular.
	
\end{itemize}

O modo mais comum de representar o processo de decisão de personagens em jogos digitais é através de máquina de estados finitos (\cite{diller2004behavior}). Ainda segundo \cite{diller2004behavior} desenvolvedores de jogos comerciais estão muito mais interessados em uma "ilusão de inteligência" (\cite{diller2004behavior}). Muitas vezes os personagens irão agir de limitados modos diferentes, por isso máquinas de estados finitos funcionam muito bem, como acontece no jogo Halo da Bungie Software, onde os guerreiros da raça Convenant(inimitogs) ficam no estado "patrulha" até perceber (por som ou visão) um jogador, em seguida, ele irá alternar para o modo ataque(\cite{Millington:2009:AIG:1795711}).

Essa técnica é muito empregada em jogos comerciais. Segundo \cite{diller2004behavior} desenvolvedores de jogos comerciais pensam em uma inteligência artificial que entre entretenha o público. Em um cenário em que o objetivo é fazer a melhor inteligência possível, ou o objetivo é vencer os melhores humanos em um determinado desafio, tais técnicas não funcionam tão bem. Em cenários competitivos pode-se enumerar alguns problemas dessas técnicas como a reatividade e a incapacidade de prever futuros movimentos ou analisar uma cadeia de movimentos em conjunto.

\subsection{Árvore de jogos}

Apesar da semelhança com árvore de decisão muitos autores como \cite{nijssen2013monte} e \cite{du1995minimax} trata como coisas diferentes mas, com recursos equivalentes. Uma árvore de jogo é uma representação de um estado em um jogo sequencial. Os nós representa as posições do jogo e as arestas representa os possíveis movimentos de uma posição. O nó raiz representa o a posição inicial e os nós folhas (ou terminais) representam o fim de jogo (\cite{nijssen2013monte}).

Os jogos podem ser classificado de acordo com algumas propriedades (\cite{browne12asurvey}):

\begin{itemize}
	\item \textbf{Soma-zero}: A soma das recompensas (positivas ou negativas) de todos os jogadores resulta em zero, ou seja, o que um jogador ganhou é exatamente o que o outro jogador perdeu.
	\item \textbf{Informação}: completo ou incompleto, perfeita ou imperfeita. Jogos com informação completa são aqueles que o estado atual do jogo é totalmente observável por todos jogadores. Informação perfeita se refere se o jogador tem a perfeita informação de tudo que já ocorreu.
	\item \textbf{Determinista}: Se existe um fator probabilístico em certas ações (também conhecida como complitude, por exemplo incerteza sobre certas recompensas).
	\item \textbf{Sequencial}: Se as ações escolhidas são aplicadas sequencialmente ou simultaneamente.
	\item \textbf{Dicreto}: discreto ou contínuo.
\end{itemize}

Ainda segundo \cite{browne12asurvey} jogos com dois jogadores que são soma-zero, de informação perfeita, determiníticos, discreto e sequenciais podem ser descritos como jogos combinatórios. Essa definição inclui jogos como: Go, Xadres e Jogo da Velha, assim como muitos outros.

\section{Minimax}
\label{sec:minimax}

O algiritmo \textit{minimax} é comumente utilizado em jogos de dois jogadores e baseados em turnos. Jogos do mundo real normalmente envolvem uma estrutura de recompensas em que, apenas as recompensas obtidas em estados terminais (jogadas que definem quem é o vencedor) descrevem com precisão o quão bem cada jogador está se saindo (\cite{browne12asurvey}).

\subsection{Conceito e algoritmo}

O algoritmo de \textit{minimax} tenta minimizar a possível perda máxima, para isso ele analisa todas as futuros possíveis movimentos de cada jogador . Os autores \cite{campbell1983comparison} descrevem o algoritmo de \textit{minimax} do seguinte modo: O algoritmo assume que existem dois jogadores chamados $Max$ e $Min$, e atribui um valor para cada nó dentro da árvore do jogo (e um particular para nó raiz) do seguinte modo: Nós folhas ou terminais propagam o valor \textit{minimax} recursivamente para todos os outros nós. Se um nó não terminal $p$ do jogador $Max$ for escolhida, então o valor de $p$ é o máximo valor dos filhos de $p$. Similarmente, se for o turno do jogador $Min$ será escolhido o menor valor dos sucessores de $p$.

O algoritmo básico do minimax \textit{minimax} pode ser representado por:

\begin{algorithm}[H]
\caption{Algoritmo Minimax}\label{euclid}
\begin{algorithmic}[1]
\Procedure{Minimax}{$node, depth, max$}
\If {$depth = 0$ \textbf{OR} node is terminal} 
	\State \Return heuristic
\EndIf
\If {$max$}
	\State $bestValue\gets -\infty$
	\For{each child \Pisymbol{psy}{206} $node$ }
		\State $value\gets \Call{Minimax}{child, depth -1, FALSE}$
		\State $bestValue\gets \Call{max}{bestValue, value}$
	\EndFor
\Else
	\State $bestValue\gets +\infty$
	\For{each child \Pisymbol{psy}{206} $node$ }
		\State $value\gets \Call{Minimax}{child, depth -1, TRUE}$
		\State $bestValue\gets \Call{min}{bestValue, value}$
	\EndFor
\EndIf
\State \Return bestValue
\EndProcedure
\end{algorithmic}
\end{algorithm}

Onde:

\begin{itemize}
	\item \textbf{node} Nó atual da árvore. Na primeira chamada é passado o nó raiz da árvore, ou seja, o estado atual do jogo.
	\item \textbf{depth} Profundidade do nó. Inicialmente é passada a altura da árvore.
	\item \textbf{max} Booleano representando se a jogada analisada é do jogador \textbf{Max} ou \textbf{Min}.
\end{itemize}

Em seu trabalho, \cite{du1995minimax} propuseram uma relação entre árvores \textit{minimax} e jogos que podem ser aplicados o \textit{minimax}. Essa relação pode ser vista na tabela \ref{tab:minimaxtreegames}.

\begin{table}[]
\centering
\caption{Árvore \textit{minimax} e jogos}
\label{tab:minimaxtreegames}
\begin{tabular}{|l|l|}
\hline
Árvore minimax                     & Árvores de jogos                                \\ \hline
Árvore                             & Todas possíveis configurações do tabuleiro      \\
Nó da árvore                       & Configuração do tabuleiro                       \\
Arestas de um nó Max até um nó Min & Movimento próprio (movimento do jogador Max)    \\
Arestas de um nó Min até um nó Max & Movimento adversário (movimento do jogador Min) \\
Valor do nó                        & A "nota" para certa configuração de tabuleiro   \\
Nó folha (terminal)                & resultado do jogo (vitória, derrota ou empate)  \\
Caminho solução                    & Sequência de Movimentos que leva a melhor saída \\ \hline
\end{tabular}
\end{table}

Na figura \ref{fig:minimaxjogodavelha} é mostrado a aplicação do algoritmo de \textit{minimax} para um cenário do jogo da velha. Na imagem o jogador Max é representado por \textbf{X} e o jogador Min por \textbf{O}. Como dito anteriormente, primeiramente a árvore é explorada (partindo da raiz) até que se chegue em nós terminais (indicando fim de jogo), em situações vitoriosas foi atribuído o valor de 10, em empates 0 e em derrota o valor -10. Em seguida, os pais desses nós folhas são preenchidos com o mínimo ou o máximo valor dos nós filhos. No primeiro nó de Min (primeiro nó da segunda fileira), por exemplo, existem dois nós filhos, um com valor 10 e outro com valor -10, por ser um nó Min é atribuído para esse nó o menor valor (no caso -10). A linha em azul mostra a melhor jogada no momento para o nó raiz.

\begin{figure}[H]
\centering
\includegraphics[width=16cm]{figures/minimax.png}
\caption[Minimax]{\textit{Minimax} aplicado a um cenário de jogo da velha.}
\label{fig:minimaxjogodavelha}
\end{figure}

Segundo \cite{browne12asurvey} existem duas variações do \textit{minimax} muito comuns de serem encontradas:

\begin{itemize}
	\item \textbf{Expectimax} generaliza \textit{minimax} para jogos estocásticos em que as transições de estado para estado são probabilística. O valor de um nó é a soma dos valores dos nós filhos ponderados por suas probabilidades (possibilidade de um estado ocorrer).
	\item \textbf{Miximax} é semelhante ao \textit{expectimax} de apenas um jogador e é usado principalmente em jogos com informações não precisas. Ele utiliza uma estratégia predefinida para tratar a decisão do oponente como nós probabilísticos.
\end{itemize}

\subsection{Problemas}

Em jogos com grande quantidade de escolhas e/ou com um grande número de turnos (altura de árvore) esses algoritmos não se mostram muito efetivos, pois cada escolha tem que ser avaliada até que chegue em nós terminais. Para a maioria dos jogos geralmente não é viável pesquisar toda a árvore até o nó terminal como especificado pelas regras do jogo (\cite{campbell1983comparison}).

Podemos enumerar dois grandes problemas ao utilizar o \textit{minimax} com uma árvore muito extensa. O primeiro deles é fazer toda a simulação de possíveis jogadas até que se chegue em nós folhas (considerando que os caminhos sejam finitos). O segundo problema ocorre quando uma árvore tem um grande número de nós e uma altura muito grande, nesse caso a função de avaliação do \textit{minimax} tem que percorrer muitos nós até que se chegue na raiz a partir dos nós folhas.

\subsection{Variações}

Uma estratégia para reduzir a quantidade de nós analisados é utilizar técnicas de poda de árvores. A poda de árvore mais conhecida na literatura é a chamada \textit{alpha-beta}. Segundo \cite{knuth1976analysis} essa é técnica é usada geralmente para aumentar a velocidade de busca sem perder informação. Nessa técnica são ignorados os nós e suas subárvores de jogadas incapazes de ser melhor do que movimentos já conhecidos. Durante a análise das jogadas são definidas duas variáveis \textit{alpha} e \textit{beta}. \textit{Alpha} representa o valor máximo que o jogador Max pode fazer e \textit{beta} a pontuação mínima do jogador Min. A cada avaliação de nó esses limites são verificados, caso o valor da avaliação não estiver entre esses limites o nó e todas suas árvores são cortados. Que pode ser dado pelo seguinte algoritmo (na primeira chamada alpha vale $-\infty$ e beta $+\infty$):

\begin{algorithm}[H]
\caption{Algoritmo alphabeta}\label{euclid}
\begin{algorithmic}[1]
\Procedure{alphabeta}{$node, depth, currentPlayer, alpha, beta$}
\If {$depth = 0$ \textbf{OR} node is terminal} 
	\State \Return heuristic
\EndIf
\If {$max$}
	
	\State $bestValue\gets -\infty$
	\For{each child \Pisymbol{psy}{206} $node$ }
		\State $bestValue\gets $\Call{max}{bestValue, \Call{alphabeta}{child, depth -1, alpha, beta, FALSE}}
		\State $alpha\gets \Call{max}{alpha, value}$
		\If {$beta \leqslant alpha$} 
			\State \textbf{break}
		\EndIf
	\EndFor
	
\Else

	\State $bestValue\gets +\infty$
	\For{each child \Pisymbol{psy}{206} $node$ }		
		\State $bestValue\gets $\Call{min}{bestValue, \Call{alphabeta}{child, depth -1, alpha, beta, TRUE}}
		\State $beta\gets \Call{min}{beta, bestValue}$
		\If {$beta \leqslant alpha$} 
			\State \textbf{break}
		\EndIf
	\EndFor
	
\EndIf
\State \Return bestValue
\EndProcedure
\end{algorithmic}
\end{algorithm}

Para resolver o problema de simular um grande número de jogadas e ao mesmo tempo diminuir a quantidade de nós a serem analisados é possível criar avaliações de heurísticas para nós não terminais e assim delimitar a simulação da árvore até uma determinada altura.

\subsection{Avaliação de heurística}

Em jogos com pequenos números de nós na árvore, a avaliação de heurística não é tão importante, um exemplo desse tipo de jogo é o jogo da velha, devido ao reduzido número de nós e a pequena altura da árvore é mais seguro explorar a árvore até o final do que criar avaliações de heurísticas para nós não terminais.

Segundo \cite{Nielsen:1994:HE:189200.189209} avaliação heurística envolve ter um pequeno conjunto de avaliadores examinando a interface e avaliando a sua conformidade em relação ao resultado. Os autores \cite{DBLP:conf/aaai/ChristensenK86} discorrem que, em jogos de tabuleiro de duas pessoas a função de heurística é vagamente caracterizado pela "força" do posicionamento de um jogador contra o outro. Ainda segundo \cite{DBLP:conf/aaai/ChristensenK86} pode-se dizer que uma função de avaliação de heurística tem duas propriedades:

\begin{itemize}
	\item Quando aplicado a um estado final (no caso de uma árvore, um nó folha), a avaliação de heurística tem que devolver o estado corretamente;
	\item O valor da função de heurística é invariável ao longo de um caminho de solução ótima.
\end{itemize}

\subsection{Sistema Deep Blue}

Um dos sistemas mais famosos que utilizou as técnicas citadas foi o Deep Blue da IBM. O Deep Blue é um sitema feito para jogar xadrez com técnicas baseadas em \textit{minimax}. Além de utilizar o buscador alphabeta, o sistema conta com uma complexa função de avaliação de nós terminais e analisava jogadas com 12 níveis de profundidade \cite{755469}. O Deep Blue era composto por um \textit{chip} chamado \textit{Chess Chip} que é dividido em três partes\cite{deepblue1}:

\begin{itemize}

	\item \textbf{Buscador alphabeta}. Menor parte do \textit{chip}, contendo apenas 5\% de seu total, esse componente é responsável por fazer a busca na árvore de jogadas. O algoritmo utilizado é uma variante do \textit{alphabeta} chamado de \textit{minimum-window alpha beta search}, essa técnica foi escolhida por não ser necessário uma pilha de valores.
	
	\item \textbf{Gerador de movimentos}. Esse componente contém o \textit{array} bidimensional 8x8 referente a cada posição do tabuleiro do xadrez, ele também é responsável por realizar e verificar a legalidade dos movimentos de acordo com as regras do xadrez.
	
	\item \textbf{Função de avaliação}. Ocupando cerca de dois terços do \textit{chip} esse componente é responsável pela avaliação dos movimentos. Cada possível posição de cada peça é avaliado, essa avaliação é baseada em quatro critérios: material, posição, segurança do rei e tempo \cite{ibmdeepblue}. Material é o quanto cada peça "vale". Por exemplo, um peão vale 1 enquanto uma torre vale 5. Posição quantifica o quão bom estão posicionadas as peças, nesse cálculo uma série de aspectos é levada em conta, um deles é o número de movimentos seguros que as peças podem fazer para atacar. Segurança do rei é calculo que leva em conta a proteção do rei. Por final tempo, além de controlar o tempo da própria jogada, fazer uma jogada rápida dá ao adversário menos tempo para pensar em possíveis jogadas.
	
\end{itemize}

\section{Árvore de Busca de Monte Carlo}
\label{sec:MCTS}

Nessa seção vamos discutir sobre o método de Monte Carlo e sua aplicação, sobre árvore de busca de Monte Carlo (do inglês Monte Carlo Tree Search - MCTS), uma heurística de bucas utilizada para estimar árvores de decisão, principalmente em jogos baseados em turnos e por fim veremos algumas melhorias e trabalhos relacionados na área. 

\subsection{Método de Monte Carlo}

O método de Monte Carlo, do qual o algoritmo MCTS foi derivado, compreende o ramo da matemática experimental que concerne com experimentos utilizando números aleatórios (\cite{hammersley2013monte}). Segundo \cite{kleij2010monte} esse método matemático se refere a utilizar amostras aleatórias com o objetivo de estimar soluções de problemas infactíveis de serem resolvidos analitcamente. 

O real uso dos metodos de Monte Calor como ferramenta de pesquisa resulta de trabalhos em bombas atômicas durante a segunda guerra, esse trabalho envolveu uma simulação direta de problemas probabilísticos relacionados com a difusão aleatória de neutrons em material físsil (\cite{hammersley2013monte}). Ainda segundo \cite{hammersley2013monte} a possibildiade de aplicar métodos de Monte Carlo para problemas determinísticos foi notado por Fermi, von Neuman, e Ulam e popularizado imedaimente após a guerra.

O método de Monte Carlo possui diversas variantes, mas todas elas seguem esses passos: Dado uma probabilida de um evento acontecer chamado de $p$, uma quantiadde de números ou conjuntos númericos aletórios são gerados, a quantidade de vezes que esse conjunto aleatório disparou o evento divivido pelo quantidade de número ou conjunto númericos gerados deve ser próximo de $p$.

\subsubsection{Utilizando método de Monte Carlo para calcular $\pi$}

Um exemplo bem difundido para utilização do método de Monte Carlo é para definir o valor de $\pi$. Calculamos a probabilidade aleatoria de pegarmos posições dentro do quadrado da figura \ref{fig:quadradoCirculoPi} e essa posição estar também dentro do círculo.

\begin{figure}[!h]
\centering
\includegraphics[width=12cm]{figures/piexample01.png}
\caption[Fluxograma estratégia Método de Monte Carlo aplicado para descobrir o valor de $\pi$ - 1]{Quadrado com lado de 2r contendo um círculo medindo r.}
\label{fig:quadradoCirculoPi}
\end{figure}

A área de um quadrao é dado pela seguinte equação:

\begin{equation}
A = l^2
\end{equation}

Onde $l$ é o lado do quadrado.

Já a área do círculo é dada pela seguinte equação:

\begin{equation}
A = \pi \times r^2
\end{equation}

Onde $r$ é o raio do círculo.

Substituindo os valores da equação temos que área do quadrado é $4r^2$ e a área do círculo é $\pi r^2$. Logo, a chance de aleatoriamente ser escolhida uma posição dentro do quadrado e essa posição também estar dentro do círculo é dado por:

\begin{equation}
p = \frac{\pi r^2}{4r^2}
\end{equation}

Isolando $\pi$ (valor que queremos descobrir) temos a seguinte equação:

\begin{equation}
\pi = 4p
\end{equation}

Com o auxílio de um algoritmo é gerado 100.000 posições aleatórias. Dessa amostra 78239 ficavam dentro do círculo. A imagem \ref{fig:quadradoCirculoPiAleatoria} faz uma demonstração gráfica desse conjunto gerado, os pontos mais escuros mostram posições geradas fora do círculo, e os pontos mais claros os pontos dentro da área do círculo.

\begin{figure}[!h]
\centering
\includegraphics[width=12cm]{figures/montecarlopi.png}
\caption[Método de Monte Carlo aplicado para descobrir o valor de $\pi$ - 2]{Demonstração gráfica de geração de 100.000 posições aleatórias.}
\label{fig:quadradoCirculoPiAleatoria}
\end{figure}

Voltando a defnição temos o seguinte: a quantidade de vezes que esse conjunto aleatório disparou o evento (78466) divivido pelo quantidade de número ou conjunto númericos gerados (100.000) deve ser próximo de $p$ ($\pi = 4p$ ou $p = \frac{\pi}{4}$. Substituindo o que descobrimos temos o seguinte:

\begin{equation}
\frac{\pi}{4} = \frac{78621}{100000}
\end{equation}

Isolando $\pi$ e resolvendo a equação temos o seguinte:

\begin{equation}
\pi = \frac{314484}{100000}
\end{equation}

Finalmente temos o valor aproximado de $\pi = 3,14484$.

\subsubsection{Utilizando método de Monte Carlo para probabilidade de uma roleta}

Com o método de Monte Carlo é bem fácil calcula probabilidade mais complexas. A imagem \ref{fig:montecarloroleta} representa uma roleta que pode ser dividida em 4 partes: em duas dessas partes o valor ganho é de $+1$, em uma dessas quatro partes o valor ganho é $+2$ e na última parte o valor é de $-2$. Nesse caso vamos usar o método de Monte Carlo para descobrir qual a probabilidade de após girar 20 vezes a roleta a soma dos pontos conseguidos ser menor que $0$.

\begin{figure}[!h]
\centering
\includegraphics[width=10cm]{figures/roleta.png}
\caption[Método de Monte Carlo aplicado a probabilidade de uma roleta]{Distrubuição de pontos de uma roleta.}
\label{fig:montecarloroleta}
\end{figure}

Será feito 1.000.000 de simulações, em cada simulação será girado a roleta 20 vezes e os pontos serão somado. Após a simulações temos o seguinte que em 64020 das simulações o resultado da soma resultou em valores negativos. Voltando a definição temos:

\begin{equation}
p = \frac{64020}{100000}
\end{equation}

Ou seja, a chance de após rodar 20 vezes a roleta da figura \ref{fig:montecarloroleta} a chance da soma de pontos ser negativa é de $0,06402$.

\subsection{Avaliação de nós utilizando Monte Carlo}

O primeiro contato do método de Monte Carlo com árvore busca foi na avaliação de nós (\cite{nijssen2013monte}, \cite{Winands2008}). Essa avaliação era chamada de avaliação de Monte Carlo (do inglês Monte Carlo Evaluation - MCE).

Em vez de usar recursos dependentes do domínio, tais como a mobilidade ou vantagem de um material, uma série de \textit{playouts} (chamado também de amostras, lançamentos, reprodução ou simulações) é iniciado a partir da posição a ser avaliada. O \textit{playout} é uma sequência de movimentos (semi-)aleatórios. O \textit{playout} termina quando o jogo está terminado e um vencedor pode ser determinado de acordo com as regras do jogo. A avaliação de uma posição é determinada por combinação dos resultados de todos os \textit{playouts} usando uma função de agregação estatística. Isso pode ser, por exemplo, a pontuação média jogo (\cite{nijssen2013monte}).

Por não exigir muito conhecimento do domínio ela funciona muito bem em jogos onde não existe uma função de avaliação de nós intermediários muito bom (como em Go por exemplo), como no trabalho de \cite{brugmann1993monte} onde o MCE foi aplicado a um tabuleiro de 9x9. A grande desvantagem dessa técnica é a quantidade de tempo que ela leva para avaliar um determinado nó até chegar em um nó folha, fazendo ela ser eficaz apenas em árvores com profundidade de nós baixa. Uma derivação direta dessa abordagem é a árvore de busca de Monte Carlo.

\subsection{Monte Carlo em árvores de busca}

Por décadas a busca alpha-beta tem sido a abordagem padrão usado em programas de jogos soma-zero de dois jogadores como xadrez e damas (entre outros). Ao passar dos anos muitas melhorias foram propostas para essa solução. Porém, em alguns jogos onde é difícil construir uma função de avaliação precisa (e.g., Go) a abordagem alpha-beta foi mal sucedida (\cite{Winands2008}).

O Algoritmo de busca de Monte Carlo ganhou muita notoriedade por apresentar uma alta competitividade ao enfrentar os melhores jogadores do jogo Go. Go é um jogo muito difícil para computadores jogar: tem alto fator de ramificação, a uma árvore profunda, e não tem nenhuma função de validação de herurística confiável para nós não terminais (\cite{browne12asurvey}). Ainda segundo os autores nos últimos anos, MCTS tem alcançado grande sucesso em muitos jogos específicos, jogos generalistas, complexos planejamentos do mundo real, otimização e controle de probelmas, e se tornou uma importante ferramenta do pesquisador de inteligência artificial.

Outro grande ponto do algoritmo de busca de Monte Carlo é o resultado positivo que ele obteve em jogos com informação incompleta e/ou imperfeita como nos trabalhos de \cite{Cai:2005:MCA:1066384.1066386} e \cite{misra2013markov}.

O método de árvore de busca de Monte Carlo se refere a encontrar soluções ótimas em um determinado domínio utilizando de amostras aleatórias no espaço de decisão e construindo uma árvore de busca de acordo com os resultados (\cite{browne12asurvey}).

A árvore de busca de Monte Carlo é um algoritmo baseado em simulação, frequentemente usado em jogos. A ideia principal é iterativamente rodar simulações do nó raiz da árvore até um nó terminal, incrementalmente crescendo uma árvore onde o nó raiz é o estado atual do jogo (\cite{tak2014monte}).

O algoritmo de MCTS é um processo iterativo que ocorre até que um determinado critério de parada seja alcançado. Segundo \cite{browne12asurvey}, geralmente essa iteração é limitada por algum recurso computacional como tempo, memória ou algum contador de iteração. 

Basicamente o MCTS é uma árvore que cresce de modo incremental e assíncrono como mostra a figura \ref{fig:mctsBasic}. O que define a expansão das árvores são as seguintes políticas:

\begin{itemize}
	\item \textbf{Política de árvore}: é a estratégia de qual nó escolher, segundo \cite{browne12asurvey} essa política tem que balancear escolher nós pouco visitados e aprofundar nós que parecem promissores. Um nó é expansível quando ele não é nó folha e ele ainda não foi visitado.
	
	\item \textbf{Política padrão}:  é a estratégia de como rodar as simulações até sua conclusão. Um caso bem simples de política padrão é fazer movimento aleatórios.
\end{itemize}


\begin{figure}[!h]
\centering
\includegraphics[width=14cm]{figures/mctsscheme.png}
\caption[Representação Básica do MCTS]{Representação Básica do MCTS \cite{5672398}.}
\label{fig:mctsBasic}
\end{figure}

Pode-se descrever cada iteração do algoritmo em quatro passos básicos:

\begin{itemize}
	
	\item \textbf{Seleção} Começando pelo nó raiz da árvore, um nó filho é escolhido de acordo com a \textbf{política de árvore} até que seja encontrado um \textbf{nó expansível}. 
	
	\item \textbf{Expansão} É adicionado um ou mais nós filhos no nó escolhido de acordo com as possíveis ações naquele estado.

	\item \textbf{Simulação} Simula o jogo a partir do(s) novo(s) nó(s) gerado(s) na fase de expansão até um o fim de jogo. As escolhas das ações a partir desse novo nó é definido pela \textbf{política padrão}. Conhecimentos do jogo pode ser adicionado para a escolha dessas ações fazendo a simulação mais realística (\cite{nijssen2013monte}).
	
	\item \textbf{Retropropagação} O resultado (valor) da simulação é propagado para todos os pais do nó folha (até o nó raiz) atualizando suas estatísticas. Normalmente os valores são: vitória 1, empate $\frac{1}{2}$ e derrota 0.
	
\end{itemize}

A imagem \ref{fig:mctsScheme} mostra o processo do MCTS. Na imagem $V$ é o valor da recompensa (vitória, empate ou derrota) de um nó folha e $\Delta$ é o resultado da estatísticas do nó (combinação de recompensas positivas e negativas).

\begin{figure}[!h]
\centering
\includegraphics[width=\columnwidth]{figures/mctsprocess.pdf}
\caption[MCTS esquema]{MCTS esquema.}
\label{fig:mctsScheme}
\end{figure}

O algoritmo a seguir mostra o procedimento básico do MCTS:

\begin{algorithm}[H]
\caption{Algoritmo MCTS}
\label{alg:mcts}
\begin{algorithmic}[1]

\Procedure{Selecionar}{$no$}
	\While{ $no.filhos > 0$ }
		\State $no\gets$ \Call{PoliticaArvore}{$no$}
	\EndWhile	
	
	\State \Return $no$
\EndProcedure
\State
\Procedure{Simular}{$no$}
	\State $acao\gets no.acao$
	\While{ \textbf{not} $acao.isFimDeJogo$}
		\State \Call{Executar}{$acao$}
		\State $acoes\gets$ \Call{LerAcoes}{ }
		\State $acao\gets$ \Call{EscolherAcaoAleatoria}{acoes}
	\EndWhile
	
	\State $valor\gets$ $\frac{1}{2}$
	\If{ $acao.isVitoria$ }
		\State $valor\gets$ $1$
	\ElsIf{ $acao.isDerrota$ }
		\State $valor\gets$ $0$
	\EndIf

	\State \Return $valor$
\EndProcedure
\State
\Procedure{MCTS}{$noAtual$}
	\While{recurso computacional}
		\State $noSelecao\gets$ \Call{Selecionar}{$noAtual$}
		\State $noExpansao\gets$ \Call{Expandir}{$noSelecao$}
		\State $valorResultado\gets$ \Call{Simular}{$noExpansao$}
		\State \Call{RetroPropagar}{$noExpansao$, $valorResultado$}
	\EndWhile
	
	\State \Return \Call{MelhorFilho}{noAtual}
\EndProcedure
\end{algorithmic}
\end{algorithm}

Onde:

\begin{itemize}
	
	\item $noAtual$: Nó que representa o estado atual do jogo. No primeiro turno é passado o nó raiz da árvore.
	
	\item \Call{MelhorFilho}{no}: Escolhe o melhor nó filho. O conceito de melhor filho varia entre as diferentes implementações do MCTS.
	
\end{itemize}

Segundo \cite{schadd2009monte} ao finalizar o MCTS existe quatro critérios de como escolher o melhor filho:

\begin{itemize}
	
	\item \textbf{Filho máximo}: Seleciona o filho com valor mais alto.
	
	\item \textbf{Filho robusto}: Seleciona o filho com o maior número de visitas.
	
	\item \textbf{Filho máximo-robusto}: Selecione o filho que tem tanto o maior número de visita e o valor mais alto. Se não existir esse filho, é melhor continuar a busca até achar um filho máximo-robusto do que escolher um filho com baixo número de visitas.
	
	\item \textbf{Filho seguro}: Selecione o filho com menor chance de ter resultado negativo.
	
\end{itemize}

\subsection{Upper Confidence Bound for Trees (UCT)}

A implementação mais comum do algoritmo MCTS é o \textit{Upper Confidence Bound for Trees} (UCT) (\cite{browne12asurvey}). 

Um dos grandes dilemas na definição de política de árvore é como balancear as escolhas entre explorar novos nós para fugir de máximos locais e aprofundar de uma subárvore existente podendo achar melhores resultados ou garantindo uma maior robustez na avaliação de um nó. Segundo \cite{auer2002finite} o Upper Confidence Bound for Trees (UCT) resolve essa questão de maneira ótima até um fator constante.

O algoritmo UCT pode ser decomposto em duas partes: MCTS e Upper Confidence Bounds (UCB). Sua primeira implementação formal foi em 2006 por Kocsis e Szepesvari \cite{kocsis2006bandit}. O algoritmo UCB entra como uma política de árvore para implementação de MCTS.


\subsubsection{Upper Confidence Bounds (UCB)}

O algoritmo mais tradicional de Upper Confidence Bounds é o UCB1 (\cite{auer2002finite}) que foi inicialmente aplicado para o problema \textit{multiarmed bandit}. Segundo \cite{auer2002using} o termo \textit{multiarmed bandit problem} ou problema dos caça-níqueis (ou mais precisamente "problema dos K caça-níqueis") reflete o problema de um apostador em uma sala com várias máquinas de caça níqueis. Em cada tentativa o apostador tem que decidir em qual máquina ele quer jogar. Para maximizar o ganho total ou recompensa, sua escolha se baseia em recompensas anteriormente coletadas em cada máquina.

O UCB1 pode ser definido pela seguitne equacao:

\begin{equation}
UCB1 = \overline{x}_{j} + \sqrt{\frac{2 \ln n}{n_{j}}}
\end{equation}

Onde: $\overline{x}_{j}$ é a média da recompensa paga pela máquina $j$, $n_{j}$ é o numero de vezes em que foi jogado na máquina $j$ e $n$ é a soma de quantas vezes foram em jogadas em todas as máquinas.

\begin{itemize}
	
	\item: $j$: a escolha que está sendo analisada. No exemplo a máquina caça-níquel.
	
	\item: $\overline{x}_{j}$: a média da recompensa paga pela máquina $j$.
	
	\item $n_{j}$: a quantidade de vezes que foi jogado na máquian $j$.
	
	\item $n$: A soma de quantas vezes foi jogado em cada máquina.
	
\end{itemize}

Nessa equação fica claro como está distribuído o fator exploração ($\sqrt{\frac{2 \ln n}{n_{j}}}$) e o fato aprofundamento ($\overline{x}_{j}$). A imagem \ref{fig:exploreandexploitinequation} ilustra essa separação.

\begin{figure}[!h]
\centering
\includegraphics[width=12cm]{figures/exploreandexploit.png}
\caption[UCB1 exploração e aprofundamento na equação]{UCB1 exploração e aprofundamento na equação.}
\label{fig:exploreandexploitinequation}
\end{figure}

Por esse motivo é comum encontrar na literatura a equação UCB1 da seguinte forma:

\begin{equation}
UCB1 = \overline{x}_{j} + C \sqrt{\frac{2 \ln n}{n_{j}}}
\end{equation}

Onde $C$ é uma constante que regula o valor da exploração, ou seja, caso queira aumentar a exploração basta $C > 1$, caso queira diminuir a exploração $1 > C > 0$.

\subsubsection{Política de árvore UCB1}

A aplicação do UCB1 como política de árvore funciona do seguinte modo: no processo de seleção a escolha pode ser modelada com um problema \textit{multiarmed bandit} independente. A utilizicação do MCTS com qualquer política de árvore UCB é chamada de UCT.

Uma variação proposta por \cite{kocsis2006improved} chamada de \textit{plain UCT} provou ter resultados muito superiores ao UCT comum. Ainda segundo \cite{kocsis2006improved} com essa modificação a chance de selecionar o melhor movimento  converge em 1 e, através de testes empíricos foi observado que é mais rápido que outros algoritmos de busca em árvore como o \textit{alpha-beta}.

A equação do \textit{plain UCT} é definida da seguinte forma:

\begin{equation}
UCT = \overline{x}_{j} + 2C_{p} \sqrt{\frac{2 \ln n}{n_{j}}}
\end{equation}

Onde $C_{p} > 0$ é uma constante. O valor é sugerido como $C_{p} = \frac{1}{\sqrt{2}}$ e pode ser ajustado para mais ou menos para regular o nível de exploração.

\subsubsection{Algoritmo UCT}

Baseado no algoritmo MCTS previamentes explicado, vamos fazer algumas alterações para implementgar o UCT. Primeiro o código do método principal:

\begin{algorithm}[H]
\caption{Algoritmo UCT}
\label{alg:mcts}
\begin{algorithmic}[1]
\Procedure{UCT}{$noAtual$}
	\While{recurso computacional}
		\State $noSelecao\gets$ \Call{Selecionar}{$noAtual$}
		\State $noExpansao\gets$ \Call{Expandir}{$noSelecao$}
		\State $valorResultado\gets$ \Call{Simular}{$noExpansao$}
		\State \Call{RetroPropagar}{$noExpansao$, $valorResultado$}
	\EndWhile
	
	\State \Return \Call{UCB1}{noAtual}
\EndProcedure
\end{algorithmic}
\end{algorithm}

Método Selecionar e UCB1:

\begin{algorithm}[H]
\caption{Algoritmo UCT - Selecionar e UCB1}
\label{alg:mcts}
\begin{algorithmic}[1]
\Procedure{UCB1}{$no$}
	\State $melhorValor\gets -\infty$ 
	\State $melhorFilho\gets$ \textbf{null}
	\For{each child \Pisymbol{psy}{206} $no$ }
		\State $valor\gets child.value + C \sqrt{\frac{2 \ln no.visitas}{child.visitas}}$
		\If{ $valor > melhorValor$ }
			\State $melhorValor\gets valor$
			\State $melhorFilho\gets child$
		\EndIf
	\EndFor
	
	\State \Return $melhorFilho$
\EndProcedure
\State
\Procedure{Selecionar}{$no$}
	\While{ $no.filhos > 0$ }
		\State $no\gets$ \Call{UCB1}{$no$}
	\EndWhile
	
	\State \Return $no$
\EndProcedure
\end{algorithmic}
\end{algorithm}

E por fim o retropropagar:

\begin{algorithm}[H]
\caption{Algoritmo UCT - Retropropagar}
\label{alg:mcts}
\begin{algorithmic}[1]
\Procedure{Retropropagar}{$no$, $valor$}
	\State $no.visitas\gets no.visitas + 1$ 
	\State $no.valor\gets no.valor + valor$

	\If{ no.pai \textbf{not} null }
		\State \Call{Retropropagar}{$no.pai$, $valor$}
	\EndIf

\EndProcedure
\State
\end{algorithmic}
\end{algorithm}

\subsubsection{Exemplo UCT}

Dado o algoritmo explicado na subseção anterior, vamos executar um exemplo. Partindo da seguinte árvore da figura \ref{fig:mctsexample01} será executado 1 iteração passo a passo. Na imagem os valores dentro do nó representam valor (soma das recompensas) / visitas (quantas vezes o nó foi visitado).

\begin{figure}[H]
\centering
\includegraphics[width=10cm]{figures/mctsexample01.pdf}
\caption[MCTS Exemplo - Árvore inicial]{MCTS Exemplo - Árvore inicial.}
\label{fig:mctsexample01}
\end{figure}

Começando pelo nó raiz chamamos a função UCT (será utilizado $C = \sqrt{2}$). Aplicando o método UCB1 a todos filhos do nó raiz a árvore fica do seguinte modo \ref{fig:mctsexample02} (valores de UCB1 estão a esquerda da árvore).

\begin{figure}[H]
\centering
\includegraphics[width=10cm]{figures/mctsexample02.pdf}
\caption[MCTS Exemplo - UCB1 aplicado]{MCTS Exemplo - UCB1 aplicado.}
\label{fig:mctsexample02}
\end{figure}

O nó com maior valor de UCB1 não tem filhos, ou seja, ele é expansível. Expandido ele e fazendo a simulação teremos a seguinte árvore como indica a figura \ref{fig:mctsexample03} (a simulação resultou em vitória).

\begin{figure}[H]
\centering
\includegraphics[width=10cm]{figures/mctsexample03.pdf}
\caption[MCTS Exemplo - Expansão e simulação]{MCTS Exemplo - Expansão e simulação.}
\label{fig:mctsexample03}
\end{figure}

O último passo da nossa iteração é a retropopagação. A imagem \ref{fig:mctsexample04} mostra como ficou a árvore depois da última adição e simulação.

\begin{figure}[H]
\centering
\includegraphics[width=10cm]{figures/mctsexample04.pdf}
\caption[MCTS Exemplo - Expansão e simulação]{MCTS Exemplo - Expansão e simulação.}
\label{fig:mctsexample04}
\end{figure}

Após isso voltamos para o início do laço e caso ainda tenha recurso computacional e começamos novamente utilzando a nova árvore.

\subsection{Melhorias MCTS}

Um aprimoramento para o UCT para quando se tem muitas ações parecidas é a chamada grupos de movimentos. Proposto por \cite{childs2008transpositions} essa melhoria diminui consideravelmente a quantidade de ramos da árvore Monte Carlo. A técnica consiste em criar uma nova camada onde todas as possíveis ações são separadas em grupos e o UCB1 é usado para selecionar qual grupo será escolhido.
